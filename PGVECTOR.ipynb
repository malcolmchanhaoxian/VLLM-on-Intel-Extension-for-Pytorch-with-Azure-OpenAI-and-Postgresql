{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6e52fe6-53fd-402f-879b-93bf88b7f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import PGVector\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from PyPDF2 import PdfReader\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5d31a33-15af-4d64-96a0-8d2c7e7826ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"IEO-Q3-2024-EN.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0441f06-25c0-4679-ba91-e92c3b0748db",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0a5c31a-4d6a-4870-bd72-f75d9ef942ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    # dimensions: Optional[int] = None, # Can specify dimensions with new text-embedding-3 models\n",
    "    azure_endpoint=,\n",
    "    api_key=,\n",
    "    openai_api_version=\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f75fc15f-323e-4365-8262-516212d8e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \n",
    "password = \n",
    "host = 'pgvector-vllm.postgres.database.azure.com'\n",
    "port = '5432'\n",
    "dbname = 'jakarta'\n",
    "\n",
    "CONNECTION_STRING = f'postgresql+psycopg2://{username}:{password}@{host}:{port}/{dbname}'\n",
    "COLLECTION_NAME = 'jakarta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5ed2ea3-2870-45a6-9ca1-97d1200a2cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/vllmclient/lib/python3.12/site-packages/langchain_community/vectorstores/pgvector.py:487: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  store = cls(\n"
     ]
    }
   ],
   "source": [
    "db = PGVector.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=docs,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54b30461-69a8-475d-992c-de466f9a3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url='http://104.43.91.226:8000/v1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62463ea1-96f5-45cd-b886-d5e54a240fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl: (3) URL rejected: Bad hostname\n",
      "{\"object\":\"list\",\"data\":[{\"id\":\"meta-llama/Llama-3.1-8B-Instruct\",\"object\":\"model\",\"created\":1731158457,\"owned_by\":\"vllm\",\"root\":\"meta-llama/Llama-3.1-8B-Instruct\",\"parent\":null,\"max_model_len\":131072,\"permission\":[{\"id\":\"modelperm-19fa43bc61324a1faf4a0d91cb168774\",\"object\":\"model_permission\",\"created\":1731158457,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
     ]
    }
   ],
   "source": [
    "!curl = http://104.43.91.226:8000/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "967b0b4e-4fa7-4a60-a1d2-24b61fa74e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2548a-697d-4ab2-a14b-f73e8fc01d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/vllmclient/lib/python3.12/site-packages/gradio/components/chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://35ce0ff4016b578599.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://35ce0ff4016b578599.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def user(user_prompt, history):\n",
    "    return \"\", history + [[user_prompt, None]]\n",
    "\n",
    "def generate_response(rag_selection, history):\n",
    "    user_message = history[-1][0]\n",
    "\n",
    "    if rag_selection == \"Direct Output with LLM\":\n",
    "      response = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    else:\n",
    "      retrieved_documents = retriever.invoke(user_message)\n",
    "      context = ' '.join([retrieved_documents[i].page_content for i in range(len(retrieved_documents))])\n",
    "      prompt = \"\"\"You are a financial assistant that specialise on Indonesia economy. Your taks is to answer the user's question to the best of your ability.\\\n",
    "                  Use the following pieces of retrieved context to answer the question only if the retrieved context is highly relevant. \\\n",
    "                  If the retrieved context is not relevant, answer the question to your best ability. \\\n",
    "\n",
    "                  {context}\"\"\"\n",
    "\n",
    "      response = [{\"role\": \"system\", \"content\": prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_message}]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model='meta-llama/Llama-3.1-8B-Instruct',  # Model name to use\n",
    "        messages=response,  # Chat history\n",
    "        stream=True,\n",
    "        stream_options={\"include_usage\": True},\n",
    "        max_tokens=128,  # Maximum number of tokens to generate\n",
    "        temperature=0.8,  # Temperature for text generation\n",
    "    )\n",
    "\n",
    "    bot_response = \"\"\n",
    "    tokens_num = 0\n",
    "    st = time.time()\n",
    "\n",
    "    for chunk in completion:\n",
    "      if hasattr(chunk.choices[0], 'finish_reason') and chunk.choices[0].finish_reason is not None:\n",
    "        next_chunk = next(completion, None)\n",
    "        if next_chunk:\n",
    "          tokens_num = next_chunk.usage.completion_tokens\n",
    "          et = time.time() - st\n",
    "          yield history, f'{round(tokens_num/et,2)}'\n",
    "        else:\n",
    "          return None # Handle the case where there's no next chunk\n",
    "      else:\n",
    "        et = time.time() - st\n",
    "        chunk_message = chunk.choices[0].delta.content or \"\"\n",
    "        bot_response += chunk_message\n",
    "        history[-1][1] = bot_response\n",
    "        yield history, f'{round(tokens_num/et,2)}'\n",
    "\n",
    "with gr.Blocks(theme=gr.Theme.from_hub('HaleyCH/HaleyCH_Theme'), css=\".column-form .wrap {flex-direction: column;}\") as demo:\n",
    "    with gr.Row():\n",
    "      gr.Markdown(\"\"\"\n",
    "            <h1><center>Intel Chatbot with vLLM Model Serving\n",
    "            <center><img src=\"https://upload.wikimedia.org/wikipedia/commons/6/64/Intel-logo-2022.png\" width=200px>\n",
    "            <h2>Inferenced on Azure instance powered by Intel 4th Gen Xeon with AMX Acceleration</h2></n>Using Meta Llama 3.1 8B</center>\n",
    "            \"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(visible=True, min_width=250, scale=0) as sidebar:\n",
    "          with gr.Row():\n",
    "            gr.Markdown(\"\"\"\n",
    "              This is a Chatbot that is supported by additional context from PDF<br>\"\"\")\n",
    "          with gr.Row():\n",
    "            dropdown = gr.Dropdown([\"Direct Output with LLM\", \"PDF-Augmented Output\"], label=\"Retrieval-Augmented Generation\")\n",
    "          with gr.Row():\n",
    "            gr.Markdown(\"\"\"    \n",
    "              - By default, the chatbot will use the base model.\n",
    "              - If PDF-Augmented Output is selected, the system will answer based on the PDF<br>              \n",
    "              \"\"\")\n",
    "          with gr.Row():\n",
    "            gr.Markdown(\"\"\"\n",
    "            **Brochure:**<br>\n",
    "            **Indonesia Economic Outlook Q3 '24**<br>\n",
    "            <img src=\"https://github.com/user-attachments/assets/85ed6ac7-1018-46b4-915a-e856c46a04c9\" width=150px>\n",
    "            \"\"\")\n",
    "\n",
    "        with gr.Column() as main:\n",
    "            with gr.Row():\n",
    "                chatbot = gr.Chatbot(label=\"Conversation\")\n",
    "            with gr.Row():\n",
    "                prompt_input = gr.Textbox(label=\"Enter your message here\", placeholder=\"Type your message...\",scale=7)\n",
    "                token_output = gr.Textbox(label=\"Tokens/sec\", placeholder=\"0.00\",scale=3)\n",
    "\n",
    "                prompt_input.submit(user, [prompt_input, chatbot], [prompt_input, chatbot], queue=False).then(\n",
    "                generate_response, inputs=[dropdown,chatbot], outputs=[chatbot,token_output], queue=True)\n",
    "\n",
    "        with gr.Column(visible=True, min_width=250, scale=0) as examplelist:\n",
    "            gr.Examples(\n",
    "            examples=[\n",
    "            [\"Tell me about Indonesia Economic Outlook Q3 2024\"],\n",
    "            [\"How is Indonesia Economic\"],\n",
    "        ],\n",
    "        inputs=prompt_input,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    with gr.Row():\n",
    "            gr.Markdown(\"\"\"\n",
    "            <center><br><h3>An Intel Collaboration with Beny Ibrani // Malcolm Chan</h3>\n",
    "            \"\"\")\n",
    "\n",
    "demo.queue(default_concurrency_limit=100).launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc63b090-fb6a-49fd-9a6e-e82e1f8cf75f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
